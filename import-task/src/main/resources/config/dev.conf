args=[
    "re_000001",
    "SD_Z6MWD3H0"
]
sources=[
    {
        format=AVRO
        id="raw_condition"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/raw/fhir/condition/study=SD_Z6MWD3H0"
        readoptions {}
        storageid=kfdrc
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="normalized_condition"
        keys=[]
        loadtype=OverWrite
        partitionby=[]
        path="/normalized/fhir/condition/study=SD_Z6MWD3H0/release=re_000001"
        readoptions {}
        storageid=output
        table {
            database=kfdrc
            name="fhir_condition"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    }
]
sparkconf {
    "spark.databricks.delta.retentionDurationCheck.enabled"="false"
    "spark.delta.merge.repartitionBeforeWrite"="true"
    "spark.hadoop.fs.s3a.access.key"=minioadmin
    "spark.hadoop.fs.s3a.endpoint"="http://127.0.0.1:9000"
    "spark.hadoop.fs.s3a.impl"="org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access"="true"
    "spark.hadoop.fs.s3a.secret.key"=minioadmin
    "spark.master"=local
    "spark.sql.catalog.spark_catalog"="org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.sql.extensions"="io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.legacy.timeParserPolicy"=CORRECTED
    "spark.sql.mapKeyDedupPolicy"="LAST_WIN"
}
storages=[
    {
        filesystem=S3
        id=kfdrc
        path="s3a://kfdrc"
    },
    {
        filesystem=S3
        id=output
        path="s3a://output"
    }
]
