sources=[
    {
        format=PARQUET
        id="es_index_study_centric"
        keys=[]
        loadtype=OverWrite
        partitionby=[study_id, release_id]
        path="/es_index/fhir/study_centric"
        readoptions {}
        storageid=es_index
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="es_index_participant_centric"
        keys=[]
        loadtype=OverWrite
        partitionby=[study_id, release_id]
        path="/es_index/fhir/participant_centric"
        readoptions {}
        storageid=es_index
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
    {
        format=PARQUET
        id="es_index_file_centric"
        keys=[]
        loadtype=OverWrite
        partitionby=[study_id, release_id]
        path="/es_index/fhir/file_centric"
        readoptions {}
        storageid=es_index
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    }
]
sparkconf {
    "spark.databricks.delta.retentionDurationCheck.enabled"="false"
    "spark.delta.merge.repartitionBeforeWrite"="true"
    "spark.hadoop.fs.s3a.access.key"=minioadmin
    "spark.hadoop.fs.s3a.endpoint"="http://127.0.0.1:9000"
    "spark.hadoop.fs.s3a.impl"="org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access"="true"
    "spark.hadoop.fs.s3a.secret.key"=minioadmin
    "spark.master"=local
    "spark.sql.catalog.spark_catalog"="org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.sql.extensions"="io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.legacy.timeParserPolicy"=CORRECTED
    "spark.sql.mapKeyDedupPolicy"="LAST_WIN"
    "spark.network.timeout"="5m"
    "spark.executor.heartbeatInterval"="1m"
    "spark.driver.memory"="6g"
    "spark.sql.autoBroadcastJoinThreshold"="-1"
}
storages=[
    {
        filesystem=S3
        id=es_index
        path="s3a://esindex"
    }
]
